{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# Example 3: Prompt Brittleness\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Javihaus/agents_observability_bootcamp/blob/main/chapter_01_diagnosing_agent_failures/examples/example_03_prompt_brittleness.ipynb)\n",
    "\n",
    "**Instructor demonstration** - Students follow along without running code\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Demonstrate how identical semantic content produces dramatically different results when prompt format changes.\n",
    "\n",
    "**Key lesson**: LLMs are highly sensitive to format, not just content. Production systems requiring structured I/O face systematic brittleness.\n",
    "\n",
    "---\n",
    "\n",
    "## Scenario\n",
    "\n",
    "**Temporal Reasoning Task**\n",
    "- Same semantic question about meeting feasibility\n",
    "- Five different formats:\n",
    "  1. Conversational (natural language)\n",
    "  2. Structured (formal specification)\n",
    "  3. JSON (production API format)\n",
    "  4. Bullet points (documentation style)\n",
    "  5. Table (data presentation)\n",
    "\n",
    "**Hypothesis**: Accuracy will vary dramatically (30-60pp swings) despite identical information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q langchain==0.1.0 langchain-anthropic==0.1.1 anthropic==0.18.1\n",
    "!pip install -q python-dotenv pandas matplotlib\n",
    "\n",
    "print(\"Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "imports"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "# Get API key (instructor's key)\n",
    "ANTHROPIC_API_KEY = userdata.get('ANTHROPIC_API_KEY')\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-cases"
   },
   "source": [
    "## Test Cases\n",
    "\n",
    "Three temporal reasoning scenarios with clear correct answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scenarios"
   },
   "outputs": [],
   "source": [
    "# Test scenarios (current_time, meeting_duration_minutes, deadline, answer)\n",
    "scenarios = [\n",
    "    {\n",
    "        'id': 1,\n",
    "        'current_time': '2:00 PM',\n",
    "        'meeting_duration': 30,\n",
    "        'deadline': '3:00 PM',\n",
    "        'correct_answer': 'YES',\n",
    "        'reasoning': '2:00 PM + 30 min = 2:30 PM, which is before 3:00 PM deadline'\n",
    "    },\n",
    "    {\n",
    "        'id': 2,\n",
    "        'current_time': '4:45 PM',\n",
    "        'meeting_duration': 45,\n",
    "        'deadline': '5:00 PM',\n",
    "        'correct_answer': 'NO',\n",
    "        'reasoning': '4:45 PM + 45 min = 5:30 PM, which exceeds 5:00 PM deadline'\n",
    "    },\n",
    "    {\n",
    "        'id': 3,\n",
    "        'current_time': '1:15 PM',\n",
    "        'meeting_duration': 90,\n",
    "        'deadline': '2:45 PM',\n",
    "        'correct_answer': 'YES',\n",
    "        'reasoning': '1:15 PM + 90 min = 2:45 PM, exactly at deadline (acceptable)'\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(scenarios)} test scenarios\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "format-1"
   },
   "source": [
    "## Format 1: Conversational (Natural Language)\n",
    "\n",
    "This format matches training data distribution - expect high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "format-1-code"
   },
   "outputs": [],
   "source": [
    "llm = ChatAnthropic(\n",
    "    model=\"claude-sonnet-4-20250514\",\n",
    "    anthropic_api_key=ANTHROPIC_API_KEY,\n",
    "    max_tokens=100,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FORMAT 1: CONVERSATIONAL\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "format1_results = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    prompt = f\"\"\"It's currently {scenario['current_time']}. I have a meeting that will take {scenario['meeting_duration']} minutes, and I need to be done by {scenario['deadline']}.\n",
    "\n",
    "Do I have enough time for this meeting? Please respond with just YES or NO.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    answer = response.content.strip().upper()\n",
    "    correct = answer == scenario['correct_answer']\n",
    "    \n",
    "    format1_results.append(correct)\n",
    "    \n",
    "    print(f\"\\nScenario {scenario['id']}:\")\n",
    "    print(f\"  Model answer: {answer}\")\n",
    "    print(f\"  Correct answer: {scenario['correct_answer']}\")\n",
    "    print(f\"  ✓ CORRECT\" if correct else f\"  ✗ WRONG\")\n",
    "\n",
    "accuracy1 = sum(format1_results) / len(format1_results) * 100\n",
    "print(f\"\\nFormat 1 Accuracy: {accuracy1:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "format-2"
   },
   "source": [
    "## Format 2: Structured (Formal Specification)\n",
    "\n",
    "More formal, less conversational - expect accuracy drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "format-2-code"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FORMAT 2: STRUCTURED\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "format2_results = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    prompt = f\"\"\"TEMPORAL FEASIBILITY EVALUATION\n",
    "\n",
    "Given:\n",
    "  T_current = {scenario['current_time']}\n",
    "  Duration = {scenario['meeting_duration']} minutes\n",
    "  T_deadline = {scenario['deadline']}\n",
    "\n",
    "Determine: Is (T_current + Duration) ≤ T_deadline?\n",
    "\n",
    "Output: YES or NO\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a temporal reasoning system.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    answer = response.content.strip().upper()\n",
    "    # Extract YES/NO if embedded in explanation\n",
    "    if 'YES' in answer and 'NO' not in answer:\n",
    "        answer = 'YES'\n",
    "    elif 'NO' in answer:\n",
    "        answer = 'NO'\n",
    "    \n",
    "    correct = answer == scenario['correct_answer']\n",
    "    \n",
    "    format2_results.append(correct)\n",
    "    \n",
    "    print(f\"\\nScenario {scenario['id']}:\")\n",
    "    print(f\"  Model answer: {answer}\")\n",
    "    print(f\"  Correct answer: {scenario['correct_answer']}\")\n",
    "    print(f\"  ✓ CORRECT\" if correct else f\"  ✗ WRONG\")\n",
    "\n",
    "accuracy2 = sum(format2_results) / len(format2_results) * 100\n",
    "print(f\"\\nFormat 2 Accuracy: {accuracy2:.1f}%\")\n",
    "print(f\"Drop from Format 1: {accuracy1 - accuracy2:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "format-3"
   },
   "source": [
    "## Format 3: JSON (Production API Format)\n",
    "\n",
    "Common in production systems - often causes significant performance degradation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "format-3-code"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FORMAT 3: JSON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "format3_results = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    input_json = {\n",
    "        \"current_time\": scenario['current_time'],\n",
    "        \"meeting_duration_minutes\": scenario['meeting_duration'],\n",
    "        \"deadline\": scenario['deadline'],\n",
    "        \"question\": \"is_meeting_feasible\"\n",
    "    }\n",
    "    \n",
    "    prompt = f\"\"\"Process this temporal constraint query:\n",
    "\n",
    "{json.dumps(input_json, indent=2)}\n",
    "\n",
    "Return your answer in JSON format:\n",
    "{{\"feasible\": true/false, \"answer\": \"YES\"/\"NO\"}}\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a JSON API for temporal reasoning.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    \n",
    "    # Try to parse JSON response\n",
    "    try:\n",
    "        result = json.loads(response.content)\n",
    "        answer = result.get('answer', '').upper()\n",
    "    except:\n",
    "        # Fallback: extract YES/NO from text\n",
    "        answer = 'YES' if 'YES' in response.content.upper() else 'NO'\n",
    "    \n",
    "    correct = answer == scenario['correct_answer']\n",
    "    \n",
    "    format3_results.append(correct)\n",
    "    \n",
    "    print(f\"\\nScenario {scenario['id']}:\")\n",
    "    print(f\"  Model answer: {answer}\")\n",
    "    print(f\"  Correct answer: {scenario['correct_answer']}\")\n",
    "    print(f\"  ✓ CORRECT\" if correct else f\"  ✗ WRONG\")\n",
    "\n",
    "accuracy3 = sum(format3_results) / len(format3_results) * 100\n",
    "print(f\"\\nFormat 3 Accuracy: {accuracy3:.1f}%\")\n",
    "print(f\"Drop from Format 1: {accuracy1 - accuracy3:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "format-4"
   },
   "source": [
    "## Format 4: Bullet Points (Documentation Style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "format-4-code"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FORMAT 4: BULLET POINTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "format4_results = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    prompt = f\"\"\"Meeting Feasibility Check:\n",
    "\n",
    "• Current time: {scenario['current_time']}\n",
    "• Meeting duration: {scenario['meeting_duration']} minutes\n",
    "• Must finish by: {scenario['deadline']}\n",
    "• Question: Can meeting be completed before deadline?\n",
    "\n",
    "Answer with YES or NO only.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a scheduling assistant.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    answer = response.content.strip().upper()\n",
    "    if 'YES' in answer and 'NO' not in answer:\n",
    "        answer = 'YES'\n",
    "    elif 'NO' in answer:\n",
    "        answer = 'NO'\n",
    "    \n",
    "    correct = answer == scenario['correct_answer']\n",
    "    \n",
    "    format4_results.append(correct)\n",
    "    \n",
    "    print(f\"\\nScenario {scenario['id']}:\")\n",
    "    print(f\"  Model answer: {answer}\")\n",
    "    print(f\"  Correct answer: {scenario['correct_answer']}\")\n",
    "    print(f\"  ✓ CORRECT\" if correct else f\"  ✗ WRONG\")\n",
    "\n",
    "accuracy4 = sum(format4_results) / len(format4_results) * 100\n",
    "print(f\"\\nFormat 4 Accuracy: {accuracy4:.1f}%\")\n",
    "print(f\"Drop from Format 1: {accuracy1 - accuracy4:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "format-5"
   },
   "source": [
    "## Format 5: Table (Data Presentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "format-5-code"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"FORMAT 5: TABLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "format5_results = []\n",
    "\n",
    "for scenario in scenarios:\n",
    "    prompt = f\"\"\"| Parameter | Value |\n",
    "|-----------|-------|\n",
    "| Current Time | {scenario['current_time']} |\n",
    "| Meeting Duration | {scenario['meeting_duration']} min |\n",
    "| Deadline | {scenario['deadline']} |\n",
    "\n",
    "Based on the table above, can the meeting be completed before the deadline?\n",
    "\n",
    "Answer: YES or NO\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        SystemMessage(content=\"You analyze tabular data.\"),\n",
    "        HumanMessage(content=prompt)\n",
    "    ]\n",
    "    \n",
    "    response = llm.invoke(messages)\n",
    "    answer = response.content.strip().upper()\n",
    "    if 'YES' in answer and 'NO' not in answer:\n",
    "        answer = 'YES'\n",
    "    elif 'NO' in answer:\n",
    "        answer = 'NO'\n",
    "    \n",
    "    correct = answer == scenario['correct_answer']\n",
    "    \n",
    "    format5_results.append(correct)\n",
    "    \n",
    "    print(f\"\\nScenario {scenario['id']}:\")\n",
    "    print(f\"  Model answer: {answer}\")\n",
    "    print(f\"  Correct answer: {scenario['correct_answer']}\")\n",
    "    print(f\"  ✓ CORRECT\" if correct else f\"  ✗ WRONG\")\n",
    "\n",
    "accuracy5 = sum(format5_results) / len(format5_results) * 100\n",
    "print(f\"\\nFormat 5 Accuracy: {accuracy5:.1f}%\")\n",
    "print(f\"Drop from Format 1: {accuracy1 - accuracy5:.1f} percentage points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "analysis"
   },
   "source": [
    "## Brittleness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "analysis-code"
   },
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PROMPT BRITTLENESS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compile results\n",
    "results_df = pd.DataFrame({\n",
    "    'Format': ['Conversational', 'Structured', 'JSON', 'Bullets', 'Table'],\n",
    "    'Accuracy': [accuracy1, accuracy2, accuracy3, accuracy4, accuracy5]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + results_df.to_string(index=False))\n",
    "\n",
    "# Calculate metrics\n",
    "max_accuracy = results_df['Accuracy'].max()\n",
    "min_accuracy = results_df['Accuracy'].min()\n",
    "variance = results_df['Accuracy'].std()\n",
    "range_pp = max_accuracy - min_accuracy\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"BRITTLENESS METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Maximum accuracy: {max_accuracy:.1f}%\")\n",
    "print(f\"Minimum accuracy: {min_accuracy:.1f}%\")\n",
    "print(f\"Range: {range_pp:.1f} percentage points\")\n",
    "print(f\"Standard deviation: {variance:.1f}pp\")\n",
    "\n",
    "if range_pp > 20:\n",
    "    print(f\"\\n⚠ HIGH BRITTLENESS: {range_pp:.1f}pp variance across formats\")\n",
    "    print(\"This system is NOT production-ready for structured I/O.\")\n",
    "else:\n",
    "    print(f\"\\n✓ Acceptable variance: {range_pp:.1f}pp\")\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars = ax.bar(results_df['Format'], results_df['Accuracy'], \n",
    "               color=['green' if a == max_accuracy else 'orange' if a >= 70 else 'red' \n",
    "                      for a in results_df['Accuracy']])\n",
    "\n",
    "ax.set_ylabel('Accuracy (%)', fontsize=12)\n",
    "ax.set_title('Prompt Format Impact on Accuracy\\n(Identical Semantic Content)', fontsize=14)\n",
    "ax.set_ylim(0, 110)\n",
    "ax.axhline(y=100, color='green', linestyle='--', alpha=0.3, label='Perfect accuracy')\n",
    "ax.axhline(y=70, color='orange', linestyle='--', alpha=0.3, label='Acceptable threshold')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center', va='bottom')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "1. Identical information, different formats → large accuracy swings\n",
    "2. Conversational format typically performs best (matches training data)\n",
    "3. Production formats (JSON, structured) often have significantly lower accuracy\n",
    "4. This is NOT a bug - it's an architectural characteristic of LLMs\n",
    "\n",
    "Implications:\n",
    "- Cannot reliably use structured I/O without extensive testing\n",
    "- Demos (conversational) don't predict production (structured) performance\n",
    "- Need hybrid approach: conversational extraction → deterministic transformation\n",
    "\n",
    "Solution (Chapter 4):\n",
    "- Use LLM for understanding (conversational strength)\n",
    "- Use deterministic code for formatting (100% reliable)\n",
    "- Separate concerns to leverage strengths of each\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "instructor-notes"
   },
   "source": [
    "---\n",
    "\n",
    "## Instructor Notes\n",
    "\n",
    "### Teaching Strategy\n",
    "\n",
    "**Before running**: Emphasize that content is IDENTICAL\n",
    "- Same times, same durations, same deadlines\n",
    "- Only formatting differs\n",
    "- Ask students to predict which format will work best\n",
    "\n",
    "**During execution**: Point out failures as they happen\n",
    "- Highlight when simple conversational works\n",
    "- Show where structured formats fail\n",
    "- Emphasize this is on trivial math problems\n",
    "\n",
    "**After completion**: Connect to production implications\n",
    "- APIs require structured I/O (JSON, XML)\n",
    "- Compliance requires formal outputs\n",
    "- Demos use conversational (misleading!)\n",
    "\n",
    "### Common Student Questions\n",
    "\n",
    "**Q: Can we fine-tune on structured formats?**\n",
    "A: Yes, but expensive and may hurt general capability. Hybrid approach is cheaper and more reliable.\n",
    "\n",
    "**Q: What about few-shot examples in structured format?**\n",
    "A: Helps somewhat, but still brittle. Test extensively.\n",
    "\n",
    "**Q: Is this Claude-specific?**\n",
    "A: No. All autoregressive LLMs show format sensitivity. GPT-4, Llama, etc. have similar issues.\n",
    "\n",
    "**Q: How do we handle this in production?**\n",
    "A: Chapter 4 shows staged processing: conversational extraction → deterministic formatting.\n",
    "\n",
    "### Time Management\n",
    "\n",
    "- Setup: 2 minutes\n",
    "- Format 1: 2 minutes\n",
    "- Format 2: 2 minutes\n",
    "- Format 3: 2 minutes\n",
    "- Format 4: 2 minutes\n",
    "- Format 5: 2 minutes\n",
    "- Analysis: 5 minutes\n",
    "- Discussion: 5 minutes\n",
    "- **Total: 22 minutes**\n",
    "\n",
    "### Variations\n",
    "\n",
    "If time permits:\n",
    "- Test with more scenarios\n",
    "- Try different models (compare Claude vs GPT-4)\n",
    "- Show temperature sensitivity\n",
    "\n",
    "### Transition to Homework\n",
    "\n",
    "\"We've seen three major failure modes: temporal coordination, cost explosion, and prompt brittleness. Now it's your turn to diagnose a real system using the framework from Chapter 1...\""
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
